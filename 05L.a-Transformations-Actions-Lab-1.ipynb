{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Transformations & Actions Lab\n\n## Learning Objectives\nIn this lab, you will:\n- Explore data to understand contents\n- Validate data consistency\n- Derive new fields from current data format\n- Convert fields where necessary\n- Calculate aggregates\n- Save aggregate data\n\n### Skills Explored\n* Explore and use DataFrames and SQL transformations\n* Gain familiarity with various methods in `spark.sql.functions` by reading API docs\n* Review how jobs are triggered in association with actions\n* Reinforce concepts of how Spark executes logic against a data source"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Overview of the Data\n\nThis lab reuses the weather data from the previous lab.\n\nThe data include multiple entries from a selection of weather stations, including average temperatures recorded in either Fahrenheit or Celcius. The schema for the table:\n\n|ColumnName  | DataType| Description|\n|------------|---------|------------|\n|NAME        |string   | Station name |\n|STATION     |string   | Unique ID |\n|LATITUDE    |float    | Latitude |\n|LONGITUDE   |float    | Longitude |\n|ELEVATION   |float    | Elevation |\n|DATE        |date     | YYYY-MM-DD |\n|UNIT        |string   | Temperature units |\n|TAVG        |float    | Average temperature |\n\nWhile the total number of rows in this dataset would make manual exploration extremely inefficient, many aggregations on these data produce a small enough output for manual review."],"metadata":{}},{"cell_type":"markdown","source":["## Register Table and Load Data to DataFrame\n\nThe following cell re-executes the logic from the last lab and ensures all students will have the same environment.\n\n#### A Breakdown of Operations\n1. The first line drops the table `weather` if it exists. This ensures that no conflicts in the metastore will occur.\n1. The schema is defined in SQL DDL. Note that column names are provided in all caps to match the formatting of the source files.\n1. The `sourcePath` specifies the parquet data to be read. The source directory is read-only and was mounted above when running `Includes/Classroom-Setup`.\n1. The `tablePath` variable specifies where the files associated with the unmanaged table will be stored. The `userhome` portion points to a directory created with each student's username on the default object store (root DBFS) associated with the Databricks workspace.\n1. The multiline block of code that includes `spark.read...write...saveAsTable` specifies a source format, schema, and path for reading. The data from the source are copied to the destination `tablePath`, overwriting any data that may already exist in that directory. The table `weather` is registered to the specified files in the destination path using the schema provided. Note that this logic takes advantage of parquet being the default format when writing with Spark (the data written in the `tablePath` will be parquet files).\n1. The final line creates a DataFrame from the `weather` table.\n\nThe table `weather` and the DataFrame `weatherDF` share the same metadata definitions, meaning that both the schema and the files referenced are identical. This provides analogous access to the data through either the DataFrames API or Spark SQL. Changes to the data saved in the `tablePath` will be immediately reflected in subsequent queries through either of these APIs."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS weather\")\n\nschemaDDL = \"NAME STRING, STATION STRING, LATITUDE FLOAT, LONGITUDE FLOAT, ELEVATION FLOAT, DATE DATE, UNIT STRING, TAVG FLOAT\"\n\nsourcePath = \"/mnt/training/weather/StationData/stationData.parquet/\"\n\ntablePath = f\"{userhome}/weather\"\n\n(spark.read\n  .format(\"parquet\")\n  .schema(schemaDDL)\n  .load(sourcePath)\n  .write\n  .option(\"path\", tablePath)\n  .mode(\"overwrite\")\n  .saveAsTable(\"weather\"))\n\nweatherDF = spark.table(\"weather\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Import Needed Functions\n\nBased on user preference, this lab can be completed using SQL, Python, or Scala. Remember that SQL queries and the DataFrames API can be bridged by using `spark.sql`, which will return a DataFrame object.\n\nMany of the methods used for DataFrame transformations live within the `sql.functions` module. Links to the Scala and Python API docs are provided here:\n\n- [pyspark.sql.functions API docs](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n- [Scala spark.sql.functions API docs](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/sql/functions.html)\n\nNote that the methods in each will compile to the same plan on execution. Some individuals report navigation of the Scala docs to be easier, even when coding in Python.\n\nIf coding in SQL, the built-in functions can be found [here](https://spark.apache.org/docs/2.3.1/api/sql/index.html).\n\nExtending on SQL, _most_ of the operations from Hive DML are also supported; full docs [here](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML).\n\n**The cell below currently only imports the `col` function. Feel free to append to the import list and re-execute this cell, or import functions as needed later in the lab.**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col # add additional methods as a comma-separated list\n\n# Alternatively, you can import all functions by commenting out and executing the following line:\n\n# from pyspark.sql.functions import *\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["-sandbox\n## Preview 20 Lines of the Data\n\nBegin by displaying 20 lines of the data to get an idea of how the data is formatted.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> `limit`, `show`, `head`, and `take` will all accomplish this, but will trigger different numbers of jobs."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\ndisplay(weatherDF.limit(20))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NAME</th><th>STATION</th><th>LATITUDE</th><th>LONGITUDE</th><th>ELEVATION</th><th>DATE</th><th>UNIT</th><th>TAVG</th></tr></thead><tbody><tr><td>HAYWARD AIR TERMINAL, CA US</td><td>USW00093228</td><td>37.6542</td><td>-122.115</td><td>13.1</td><td>2018-05-27</td><td>F</td><td>61.0</td></tr><tr><td>BIG ROCK CALIFORNIA, CA US</td><td>USR0000CBIR</td><td>38.0394</td><td>-122.57</td><td>457.2</td><td>2018-01-05</td><td>C</td><td>11.7</td></tr><tr><td>SAN FRANCISCO INTERNATIONAL AIRPORT, CA US</td><td>USW00023234</td><td>37.6197</td><td>-122.3647</td><td>2.4</td><td>2018-02-24</td><td>C</td><td>8.3</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td><td>USR0000CTRA</td><td>37.8339</td><td>-122.0669</td><td>536.4</td><td>2018-03-26</td><td>C</td><td>9.4</td></tr><tr><td>HOUSTON INTERCONTINENTAL AIRPORT, TX US</td><td>USW00012960</td><td>29.98</td><td>-95.36</td><td>29.0</td><td>2018-05-25</td><td>F</td><td>80.0</td></tr><tr><td>BIG ROCK CALIFORNIA, CA US</td><td>USR0000CBIR</td><td>38.0394</td><td>-122.57</td><td>457.2</td><td>2018-05-16</td><td>C</td><td>11.1</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td><td>USR0000CBKD</td><td>37.95</td><td>-121.8844</td><td>487.7</td><td>2018-05-25</td><td>C</td><td>10.6</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td><td>USR0000CTRA</td><td>37.8339</td><td>-122.0669</td><td>536.4</td><td>2018-05-21</td><td>C</td><td>11.7</td></tr><tr><td>WOODACRE CALIFORNIA, CA US</td><td>USR0000CWOO</td><td>37.9906</td><td>-122.6447</td><td>426.7</td><td>2018-05-26</td><td>F</td><td>53.0</td></tr><tr><td>BRIONES CALIFORNIA, CA US</td><td>USR0000CBRI</td><td>37.9442</td><td>-122.1178</td><td>442.0</td><td>2018-04-08</td><td>F</td><td>53.0</td></tr><tr><td>SAN FRANCISCO INTERNATIONAL AIRPORT, CA US</td><td>USW00023234</td><td>37.6197</td><td>-122.3647</td><td>2.4</td><td>2018-03-26</td><td>C</td><td>11.7</td></tr><tr><td>SPRING VALLEY CALIFORNIA, CA US</td><td>USR0000CSVA</td><td>37.5625</td><td>-122.4364</td><td>327.7</td><td>2018-04-20</td><td>F</td><td>53.0</td></tr><tr><td>PULGAS CALIFORNIA, CA US</td><td>USR0000CPUL</td><td>37.475</td><td>-122.2981</td><td>196.3</td><td>2018-01-02</td><td>F</td><td>56.0</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td><td>USR0000CTRA</td><td>37.8339</td><td>-122.0669</td><td>536.4</td><td>2018-01-30</td><td>C</td><td>15.0</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td><td>USR0000CBKD</td><td>37.95</td><td>-121.8844</td><td>487.7</td><td>2018-04-30</td><td>C</td><td>10.6</td></tr><tr><td>HOUSTON WILLIAM P HOBBY AIRPORT, TX US</td><td>USW00012918</td><td>29.63806</td><td>-95.28194</td><td>13.4</td><td>2018-04-03</td><td>C</td><td>23.9</td></tr><tr><td>MERRITT ISLAND FLORIDA, FL US</td><td>USR0000FMER</td><td>28.6408</td><td>-80.7308</td><td>9.1</td><td>2018-01-01</td><td>C</td><td>13.9</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td><td>USR0000CBKD</td><td>37.95</td><td>-121.8844</td><td>487.7</td><td>2018-03-04</td><td>C</td><td>5.0</td></tr><tr><td>HOUSTON WILLIAM P HOBBY AIRPORT, TX US</td><td>USW00012918</td><td>29.63806</td><td>-95.28194</td><td>13.4</td><td>2018-02-22</td><td>C</td><td>16.1</td></tr><tr><td>MERRITT ISLAND FLORIDA, FL US</td><td>USR0000FMER</td><td>28.6408</td><td>-80.7308</td><td>9.1</td><td>2018-04-09</td><td>C</td><td>23.9</td></tr></tbody></table></div>"]}}],"execution_count":11},{"cell_type":"code","source":["%sql\n-- ANSWER\n\nSELECT * \nFROM weather\nLIMIT 20"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NAME</th><th>STATION</th><th>LATITUDE</th><th>LONGITUDE</th><th>ELEVATION</th><th>DATE</th><th>UNIT</th><th>TAVG</th></tr></thead><tbody><tr><td>HAYWARD AIR TERMINAL, CA US</td><td>USW00093228</td><td>37.6542</td><td>-122.115</td><td>13.1</td><td>2018-05-27</td><td>F</td><td>61.0</td></tr><tr><td>BIG ROCK CALIFORNIA, CA US</td><td>USR0000CBIR</td><td>38.0394</td><td>-122.57</td><td>457.2</td><td>2018-01-05</td><td>C</td><td>11.7</td></tr><tr><td>SAN FRANCISCO INTERNATIONAL AIRPORT, CA US</td><td>USW00023234</td><td>37.6197</td><td>-122.3647</td><td>2.4</td><td>2018-02-24</td><td>C</td><td>8.3</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td><td>USR0000CTRA</td><td>37.8339</td><td>-122.0669</td><td>536.4</td><td>2018-03-26</td><td>C</td><td>9.4</td></tr><tr><td>HOUSTON INTERCONTINENTAL AIRPORT, TX US</td><td>USW00012960</td><td>29.98</td><td>-95.36</td><td>29.0</td><td>2018-05-25</td><td>F</td><td>80.0</td></tr><tr><td>BIG ROCK CALIFORNIA, CA US</td><td>USR0000CBIR</td><td>38.0394</td><td>-122.57</td><td>457.2</td><td>2018-05-16</td><td>C</td><td>11.1</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td><td>USR0000CBKD</td><td>37.95</td><td>-121.8844</td><td>487.7</td><td>2018-05-25</td><td>C</td><td>10.6</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td><td>USR0000CTRA</td><td>37.8339</td><td>-122.0669</td><td>536.4</td><td>2018-05-21</td><td>C</td><td>11.7</td></tr><tr><td>WOODACRE CALIFORNIA, CA US</td><td>USR0000CWOO</td><td>37.9906</td><td>-122.6447</td><td>426.7</td><td>2018-05-26</td><td>F</td><td>53.0</td></tr><tr><td>BRIONES CALIFORNIA, CA US</td><td>USR0000CBRI</td><td>37.9442</td><td>-122.1178</td><td>442.0</td><td>2018-04-08</td><td>F</td><td>53.0</td></tr><tr><td>SAN FRANCISCO INTERNATIONAL AIRPORT, CA US</td><td>USW00023234</td><td>37.6197</td><td>-122.3647</td><td>2.4</td><td>2018-03-26</td><td>C</td><td>11.7</td></tr><tr><td>SPRING VALLEY CALIFORNIA, CA US</td><td>USR0000CSVA</td><td>37.5625</td><td>-122.4364</td><td>327.7</td><td>2018-04-20</td><td>F</td><td>53.0</td></tr><tr><td>PULGAS CALIFORNIA, CA US</td><td>USR0000CPUL</td><td>37.475</td><td>-122.2981</td><td>196.3</td><td>2018-01-02</td><td>F</td><td>56.0</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td><td>USR0000CTRA</td><td>37.8339</td><td>-122.0669</td><td>536.4</td><td>2018-01-30</td><td>C</td><td>15.0</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td><td>USR0000CBKD</td><td>37.95</td><td>-121.8844</td><td>487.7</td><td>2018-04-30</td><td>C</td><td>10.6</td></tr><tr><td>HOUSTON WILLIAM P HOBBY AIRPORT, TX US</td><td>USW00012918</td><td>29.63806</td><td>-95.28194</td><td>13.4</td><td>2018-04-03</td><td>C</td><td>23.9</td></tr><tr><td>MERRITT ISLAND FLORIDA, FL US</td><td>USR0000FMER</td><td>28.6408</td><td>-80.7308</td><td>9.1</td><td>2018-01-01</td><td>C</td><td>13.9</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td><td>USR0000CBKD</td><td>37.95</td><td>-121.8844</td><td>487.7</td><td>2018-03-04</td><td>C</td><td>5.0</td></tr><tr><td>HOUSTON WILLIAM P HOBBY AIRPORT, TX US</td><td>USW00012918</td><td>29.63806</td><td>-95.28194</td><td>13.4</td><td>2018-02-22</td><td>C</td><td>16.1</td></tr><tr><td>MERRITT ISLAND FLORIDA, FL US</td><td>USR0000FMER</td><td>28.6408</td><td>-80.7308</td><td>9.1</td><td>2018-04-09</td><td>C</td><td>23.9</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Limiting the view of the data to 20 lines is a transformation, but any time data is returned to display, an action (and **_at least 1_** job) will be triggered."],"metadata":{}},{"cell_type":"markdown","source":["## Define a New DataFrame or View Containing All Distinct Names\n\nBecause of lazy evaluation, DataFrames and views don't execute until an action is called against them.\n\nRegistering intermediate temp views or DataFrames essentially allow a set of transformations against a dataset to be given a name. This can be helpful when building up complex logic, as no data will be replicated when an intermediate state will be used multiple times in later queries.\n\nUse the `distinct` command on the `NAME` column and save the result to a new DataFame named `uniqueNamesDF`. Note that your definition should not trigger a job."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nuniqueNamesDF = weatherDF.select(\"NAME\").distinct()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["%sql\n-- ANSWER\n\nCREATE OR REPLACE TEMP VIEW unique_names\nAS (\n  SELECT DISTINCT(NAME)\n  FROM weather\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["-sandbox\nNow return the count and display the unique names. Each of these is a separate action.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> `display()` will suppress any console output, so do this in 2 cells."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ndisplay(uniqueNamesDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NAME</th></tr></thead><tbody><tr><td>BIG ROCK CALIFORNIA, CA US</td></tr><tr><td>PULGAS CALIFORNIA, CA US</td></tr><tr><td>LOS PRIETOS CALIFORNIA, CA US</td></tr><tr><td>SAN FRANCISCO INTERNATIONAL AIRPORT, CA US</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td></tr><tr><td>BARNABY CALIFORNIA, CA US</td></tr><tr><td>HOUSTON WILLIAM P HOBBY AIRPORT, TX US</td></tr><tr><td>SPRING VALLEY CALIFORNIA, CA US</td></tr><tr><td>OAKLAND NORTH CALIFORNIA, CA US</td></tr><tr><td>OAKLAND SOUTH CALIFORNIA, CA US</td></tr><tr><td>MERRITT ISLAND FLORIDA, FL US</td></tr><tr><td>WOODACRE CALIFORNIA, CA US</td></tr><tr><td>HAYWARD AIR TERMINAL, CA US</td></tr><tr><td>HOUSTON INTERCONTINENTAL AIRPORT, TX US</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td></tr><tr><td>CONCORD BUCHANAN FIELD, CA US</td></tr><tr><td>BRIONES CALIFORNIA, CA US</td></tr></tbody></table></div>"]}}],"execution_count":18},{"cell_type":"code","source":["%sql\n-- ANSWER\nSELECT * FROM unique_names"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NAME</th></tr></thead><tbody><tr><td>BIG ROCK CALIFORNIA, CA US</td></tr><tr><td>PULGAS CALIFORNIA, CA US</td></tr><tr><td>LOS PRIETOS CALIFORNIA, CA US</td></tr><tr><td>SAN FRANCISCO INTERNATIONAL AIRPORT, CA US</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td></tr><tr><td>BARNABY CALIFORNIA, CA US</td></tr><tr><td>HOUSTON WILLIAM P HOBBY AIRPORT, TX US</td></tr><tr><td>OAKLAND NORTH CALIFORNIA, CA US</td></tr><tr><td>SPRING VALLEY CALIFORNIA, CA US</td></tr><tr><td>OAKLAND SOUTH CALIFORNIA, CA US</td></tr><tr><td>MERRITT ISLAND FLORIDA, FL US</td></tr><tr><td>WOODACRE CALIFORNIA, CA US</td></tr><tr><td>HAYWARD AIR TERMINAL, CA US</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td></tr><tr><td>HOUSTON INTERCONTINENTAL AIRPORT, TX US</td></tr><tr><td>CONCORD BUCHANAN FIELD, CA US</td></tr><tr><td>BRIONES CALIFORNIA, CA US</td></tr></tbody></table></div>"]}}],"execution_count":19},{"cell_type":"code","source":["# ANSWER\nuniqueNamesDF.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: 17</div>"]}}],"execution_count":20},{"cell_type":"code","source":["%sql\n-- ANSWER\nSELECT COUNT(*) FROM unique_names"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>17</td></tr></tbody></table></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["Again, the lazy evaluation in Spark waits until these results need to be returned to trigger a job."],"metadata":{}},{"cell_type":"markdown","source":["## Confirm Station Information Consistency\nThe fields `NAME`, `STATION`, `LATITUDE`, `LONGITUDE`, and `ELEVATION` should remain consistent for each unique station throughout the data. \nIf this is true, the count of distinct names should be equivalent to the count of the distinct combinations of these five columns in the present data. Write a query that confirms this using the DataFrame or view defined in the previous step as a point of reference."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nweatherDF.select(\"NAME\", \"STATION\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\").distinct().count() == uniqueNamesDF.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: True</div>"]}}],"execution_count":24},{"cell_type":"code","source":["%sql\n-- ANSWER\nSELECT (\n  SELECT COUNT(DISTINCT NAME, STATION, LATITUDE, LONGITUDE, ELEVATION)\n  FROM weather\n  ) = (\n  SELECT COUNT(*) FROM unique_names\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>(scalarsubquery() = scalarsubquery())</th></tr></thead><tbody><tr><td>true</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["## Examine Date Range for Each Station\n\nCreate a DataFrame or view containing the earliest and latest date recorded for each station, alongside the total count of records.\n\nAfter a `groupBy`, the `agg` function will allow multiple aggregate calls in a single query.\n\nMake sure to `alias` the aggregate columns, as the default outputs will include parentheses (which aren't valid when saving to parquet)."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import min, max, count\n\nstationAggDF = (weatherDF.groupBy(\"NAME\").agg(\n  min(\"DATE\").alias(\"date_min\"),\n  max(\"DATE\").alias(\"date_max\"),\n  count(\"DATE\").alias(\"count\")))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["%sql\n-- ANSWER\n\nCREATE OR REPLACE TEMP VIEW station_agg_temp\nAS (\n  SELECT NAME, MIN(DATE) date_min, MAX(DATE) date_max, COUNT(DATE) count \n  FROM weather\n  GROUP BY NAME\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["Displaying the results of this aggregation to the notebook allows the user to manually review an interactive table of the results."],"metadata":{}},{"cell_type":"code","source":["display(stationAggDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NAME</th><th>date_min</th><th>date_max</th><th>count</th></tr></thead><tbody><tr><td>BIG ROCK CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>PULGAS CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>LOS PRIETOS CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>SAN FRANCISCO INTERNATIONAL AIRPORT, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>149</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>BARNABY CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>HOUSTON WILLIAM P HOBBY AIRPORT, TX US</td><td>2018-01-01</td><td>2018-05-31</td><td>150</td></tr><tr><td>OAKLAND NORTH CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>SPRING VALLEY CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>OAKLAND SOUTH CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>MERRITT ISLAND FLORIDA, FL US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>WOODACRE CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>HAYWARD AIR TERMINAL, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>149</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>HOUSTON INTERCONTINENTAL AIRPORT, TX US</td><td>2018-01-01</td><td>2018-05-31</td><td>150</td></tr><tr><td>CONCORD BUCHANAN FIELD, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>149</td></tr><tr><td>BRIONES CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"code","source":["%sql\n-- ANSWER\nSELECT * FROM station_agg_temp"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>NAME</th><th>date_min</th><th>date_max</th><th>count</th></tr></thead><tbody><tr><td>BIG ROCK CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>PULGAS CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>LOS PRIETOS CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>SAN FRANCISCO INTERNATIONAL AIRPORT, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>149</td></tr><tr><td>LAS TRAMPAS CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>BARNABY CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>HOUSTON WILLIAM P HOBBY AIRPORT, TX US</td><td>2018-01-01</td><td>2018-05-31</td><td>150</td></tr><tr><td>OAKLAND NORTH CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>SPRING VALLEY CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>OAKLAND SOUTH CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>MERRITT ISLAND FLORIDA, FL US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>WOODACRE CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>HAYWARD AIR TERMINAL, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>149</td></tr><tr><td>HOUSTON INTERCONTINENTAL AIRPORT, TX US</td><td>2018-01-01</td><td>2018-05-31</td><td>150</td></tr><tr><td>BLACK DIAMOND CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr><tr><td>CONCORD BUCHANAN FIELD, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>149</td></tr><tr><td>BRIONES CALIFORNIA, CA US</td><td>2018-01-01</td><td>2018-05-31</td><td>151</td></tr></tbody></table></div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["Stations have roughly the same number of records over the same 5 month period."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Save data\n\nUsing the provided path, write the data using [parquet file format](https://parquet.apache.org/), a columnar storage format that is [drastically better than CSV or JSON](https://databricks.com/session/spark-parquet-in-depth), especially when working in Spark.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> When first creating an unmanaged table with SQL, you can specify the format and location to save to. The main difference between this and using the DataFrameWriter (with `overwrite` mode) is that a table will be registered to the Hive metastore.\n\n```\nCREATE TABLE table_name\nUSING format\nLOCATION \"/path/to/directory\"\nAS SELECT * FROM final_view;```"],"metadata":{}},{"cell_type":"code","source":["#ANSWER\n\nstationAggPath = f\"{userhome}/station-agg\"\n(stationAggDF\n  .write\n  .format(\"parquet\")\n  .mode(\"overwrite\")\n  .save(stationAggPath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["# ANSWER\nspark.sql(\"DROP TABLE IF EXISTS station_agg\")\n\nspark.sql(f\"\"\"\nCREATE TABLE station_agg\nUSING PARQUET\nLOCATION \"{stationAggPath}\"\nAS SELECT * FROM station_agg_temp\n\"\"\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[19]: DataFrame[]</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["## Wait--What Data is Being Written?\n\nThroughout this notebook, many transformations and actions were called. The approach each individual took may vary, but actions were called every time:\n- data was displayed\n- counts were returned\n- results were written to disk\n\nThese **actions** are easy to spot reviewing the notebook: if a job was triggered, an action occurred.\n\n_Everything else_ is a transformation. This includes:\n- reading data\n- extracting strings\n- grouping and aggregation\n- creating new columns\n- creating views\n- defining new DataFrames from other DataFrames\n\nWhen the final write is triggered, Spark looks back to the source (here the files associated with the `weather` table) and creates a plan against those data. **All the computed values, DataFrame definitons, and views of the data returned to the notebook are ignored.** The final DataFrame still indicates a series of transformations against files on disk, just as the first transformation (the `read` operation) referred to these files. While some stages or tasks may be skipped because of implicit caching, Spark will always use your original data source as the single point of truth when building out the physical plan for execution."],"metadata":{}},{"cell_type":"markdown","source":["## Synopsis\n\nThis notebook explored:\n* How to use DataFrames and SQL transformations\n* Implementing various methods in `spark.sql.functions` (which likely required reading API docs)\n* How **actions** trigger jobs while transformations do not\n* Which data is referenced as transformations and actions are executed"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"05L.a-Transformations-Actions-Lab-1","notebookId":2616396637997840},"nbformat":4,"nbformat_minor":0}
